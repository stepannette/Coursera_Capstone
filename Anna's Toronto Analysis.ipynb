{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Segmenting and Clustering Neighborhoods in Toronto"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "_Anna A. Stepanova, Ph.D_"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Introduction"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this project, I will scrape the web data using a package *BeautifulSoup*. Then, I'll get the neighborhood information data from web, convert addresses into their equivalent latitude and longitude values. Also, I will use the Foursquare API to explore neighborhoods in Torono. I will use the **explore** function to get the most common venue categories in each neighborhood, and then use this feature to group the neighborhoods into clusters. I will use the *k*-means clustering algorithm to complete this task. Finally, I will use the Folium library to visualize the neighborhoods in Toronto and their emerging clusters."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of Contents\n\n\n1. [Scrape Wikipedia Page](#item1)\n2. [Add Geospatial Data](#item2)\n3. [Segmentation and Clustering Neighborhoods in Toronto](#item3)\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's download required packages before we explore the data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#!pip install --upgrade ibm-watson\n\nimport numpy as np # library to handle data in a vectorized manner\n\n#!pip install --user pandas==1.0.3\n\nimport pandas as pd # library for data analsysis\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\n\n!conda install -c conda-forge folium=0.11.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nfrom bs4 import BeautifulSoup # web scrapping library\n\nprint('Libraries imported.')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"item1\"></a>\n## 1. Scrape Wikipedia Page"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's build the code to scrape the following Wikipedia page, https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M, in order to obtain the data that is in the table of postal codes, boroughs and neighborhoods.\nWe'll use BeautifulSoup library to extract the table from the web-page."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# import the library we use to open URLs\nimport urllib.request\n\n# specify the URL of the Wikipedia page page we are going to be scraping\nurl = \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Using the *urllib.request* library, we want to query the page and put the HTML data into a variable (which we have called \u2018url\u2019):"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "page = urllib.request.urlopen(url)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Then we use Beautiful Soup to parse the HTML data we stored in our \u2018url\u2019 variable and store it in a new variable called \u2018soup\u2019 in the Beautiful Soup format. We use the \u201clxml\u201d library option:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# parse the HTML from our URL into the BeautifulSoup parse tree format\nsoup = BeautifulSoup(page, \"lxml\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Extract the table data from the xml using class \"wikitable sortable\":"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# find and extract table data from the Wikipedia page\ntable=soup.find('table', class_='wikitable sortable')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now that the table has been found, let's use BeautifulSoup to extract rows into 3 future columns. Then we'll use *pandas* to create a data frame. All entries end up with new lines **\\n** which we should replace prior to further analysis. We will exclude cells with a borough that is **Not assigned** and reset the indexes in a modified data frame. \nLet's print first 12 rows"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "### Let's get column data\n#Initialize the columns\nA=[]\nB=[]\nC=[]\n\n\nfor row in table.findAll('tr'):\n    cells=row.findAll('td')\n    if len(cells)==3:\n        A.append(cells[0].find(text=True))\n        B.append(cells[1].find(text=True))\n        C.append(cells[2].find(text=True))\n        \n#### Now let's create a data frame using pandas library\ntor=pd.DataFrame(A,columns=['PostalCode'])\ntor['Borough']=B\ntor['Neighborhood']=C\n\n\n# First filter out those rows which \n# does not contain any data \ntor = tor.dropna(how = 'all')\n\n# Remove \\n from data frame\ntor = tor.replace('\\n','', regex=True)\n\n### Drop rows Not assigned\ntor.drop(tor[tor['Borough'] == 'Not assigned'].index, inplace = True)\n\n### print the data frame resetting the index\ntor.reset_index(drop=True, inplace = True)\n\n### Print the modified dataframe \ntor.head(12)\n     \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The dataframe consists of three columns: PostalCode, Borough, and Neighborhood and 103 entries for these columns."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### print the shape of the data frame\nprint(tor.shape)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"item2\"></a>\n## 2. Add Geospatial Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this part, we'll get the latitude and longitude coordinates of a given postal code using *Geocoder* package: NOT WORKING"
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": "# initialize your variable to None\nlat_lng_coords = None\npostal_code = \"M5G\"\n# loop until you get the coordinates\nwhile(lat_lng_coords is None):\n  g = geocoder.google('{}, Toronto, Ontario'.format(postal_code))\n  lat_lng_coords = g.latlng\n\nlatitude = lat_lng_coords[0]\nlongitude = lat_lng_coords[1]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As I was not able to get the geographical coordinates of the neighborhoods using the Geocoder package, I used a csv file that has the geographical coordinates of each postal code: http://cocl.us/Geospatial_data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### Read Geospatial information from csv\npostal_data = pd.read_csv('http://cocl.us/Geospatial_data')\n\npostal_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we can create a new data frame containing information about neighborhoods and their coordinates by merging Neighborhood data with Geospatial data. First 12 rows printed"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Merge toronto data frame with postal_data\ngeo_tor = tor.merge(postal_data, left_on='PostalCode', right_on='Postal Code')\n\n# drop repetitive Postal Code\ngeo_tor.drop([\"Postal Code\"], axis = 1, inplace = True)\n\n# Print first 12 rows\ngeo_tor.head(12)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Our new data frame contains 103 entries and now has neighborhood coordinates."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### print the shape of the data frame\ngeo_tor.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"item3\"></a>\n## 3. Clustering Neighborhoods in Toronto"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this final section, we'll use Foursquare API to cluster neighborhoods in Toronto."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Define Foursquare Credentials"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's explore the 41st neighborhood in our Toronto data frame. The code below will print the neighborhood name and its coordinates."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# print neighborhood name\nneighborhood_name = geo_tor.loc[40, 'Neighborhood'] # neighborhood name\n\nprint('The 41st neighborhood in a cleaned Toronto data frame is {}.'.format(neighborhood_name))\n\n# find neighborhood coordinates\nneighborhood_latitude = geo_tor.loc[40, 'Latitude'] # neighborhood latitude value\nneighborhood_longitude = geo_tor.loc[40, 'Longitude'] # neighborhood longitude value\n\n\nprint('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n                                                               round(neighborhood_latitude, 2), \n                                                               round(neighborhood_longitude, 2)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Now, let's get the top 100 venues that are in Downsview within a radius of 1 kilometer."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First, let's create the GET request URL. Name your URL **url**."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# type your answer here\nsearch_query = neighborhood_name\nradius = 1000 # define the radius\nLIMIT = 100 # limit of number of venues returned by Foursquare API\nprint(search_query + ' .... OK!')\n\n\nurl = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(\n    CLIENT_ID, \n    CLIENT_SECRET, \n    neighborhood_latitude, \n    neighborhood_longitude, \n    VERSION,\n    radius, \n    LIMIT)\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we are ready to send our GET request and examine the resutls"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "results = requests.get(url).json()\nresults"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Before we proceed, let's borrow the get_category_type function from the Foursquare lab."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# function that extracts the category of the venue\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we are ready to clean the json and structure it into a *pandas* dataframe."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "venues = results['response']['groups'][0]['items']\n    \nnearby_venues = pd.json_normalize(venues) # flatten JSON\n\n# filter columns\nfiltered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\nnearby_venues = nearby_venues[filtered_columns]\n\n# filter the category for each row\nnearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n\n# clean columns\nnearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n\nnearby_venues.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}